{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/content/drive/My Drive/rossmann-store-sales/train.csv', low_memory=False)\n",
    "test_df = pd.read_csv('/content/drive/My Drive/rossmann-store-sales/test.csv', low_memory=False)\n",
    "store_df = pd.read_csv('/content/drive/My Drive/rossmann-store-sales/store.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge train and test data with store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.merge(store_df, on='Store', how='left')\n",
    "test_data = test_df.merge(store_df, on='Store', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop unnecessary columns and Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop([\"Customers\", \"PromoInterval\"], axis=1, inplace=True)\n",
    "test_data.drop([\"Id\", \"PromoInterval\"], axis=1, inplace=True)\n",
    " \n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Features from Datetime Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_features(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['IsWeekend'] = df['Date'].dt.dayofweek >= 5\n",
    "    df['IsMonthStart'] = df['Date'].dt.is_month_start\n",
    "    df['IsMonthEnd'] = df['Date'].dt.is_month_end\n",
    "    df.drop('Date', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_data = extract_date_features(train_data)\n",
    "test_data = extract_date_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['StateHoliday'] = train_data['StateHoliday'].replace({'0': 0, 'a': 1, 'b': 2, 'c': 3}).astype(int)\n",
    "train_data['StoreType'] = train_data['StoreType'].map({'a': 1, 'b': 2, 'c': 3, 'd': 4})\n",
    "train_data['Assortment'] = train_data['Assortment'].map({'a': 1, 'b': 2, 'c': 3})\n",
    "\n",
    "test_data['StateHoliday'] = test_data['StateHoliday'].replace({'0': 0, 'a': 1, 'b': 2, 'c': 3}).astype(int)\n",
    "test_data['StoreType'] = test_data['StoreType'].map({'a': 1, 'b': 2, 'c': 3, 'd': 4})\n",
    "test_data['Assortment'] = test_data['Assortment'].map({'a': 1, 'b': 2, 'c': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_features = train_data.drop('Sales', axis=1)\n",
    "train_labels = train_data['Sales']\n",
    "scaled_train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "test_features = test_data\n",
    "scaled_test_features = scaler.transform(test_features)\n",
    "\n",
    "# Convert scaled features back to DataFrame for convenience\n",
    "scaled_train_df = pd.DataFrame(scaled_train_features, columns=train_features.columns)\n",
    "scaled_test_df = pd.DataFrame(scaled_test_features, columns=test_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building Models with Sklearn Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and Train the Model by Using RandomForrestRegressor within Sklearn Pipelines\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(scaled_train_df, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = pipeline.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Validation MSE: {mse}\")\n",
    "\n",
    "# Train the model on the full training set\n",
    "pipeline.fit(scaled_train_df, train_labels)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = pipeline.predict(scaled_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose a Loss Function\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "importances = pipeline.named_steps['model'].feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': train_features.columns, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confidence Interval Estimation\n",
    "\n",
    "# Estimating confidence intervals using bootstrapping\n",
    "# Number of bootstrap samples\n",
    "from sklearn.utils import resample\n",
    "\n",
    "n_bootstraps = 1\n",
    "bootstrap_preds = np.zeros((n_bootstraps, len(X_val)))\n",
    "\n",
    "# Generate bootstrap samples, train, and predict\n",
    "for i in range(n_bootstraps):\n",
    "    X_train_bootstrap, y_train_bootstrap = resample(X_train, y_train, random_state=i)\n",
    "    pipeline.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "    bootstrap_preds[i] = pipeline.predict(X_val)\n",
    "\n",
    "# Calculate the confidence intervals\n",
    "lower_percentile = 2.5\n",
    "upper_percentile = 97.5\n",
    "lower_bound = np.percentile(bootstrap_preds, lower_percentile, axis=0)\n",
    "upper_bound = np.percentile(bootstrap_preds, upper_percentile, axis=0)\n",
    "\n",
    "# Calculate the mean predictions\n",
    "y_pred_mean = np.mean(bootstrap_preds, axis=0)\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_val,\n",
    "    'Predicted Mean': y_pred_mean,\n",
    "    'Lower Bound': lower_bound,\n",
    "    'Upper Bound': upper_bound\n",
    "})\n",
    "\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize Models\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# Define the file path\n",
    "model_filename = f'model_{timestamp}.pkl'\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, model_filename)\n",
    "\n",
    "print(f'Model saved as {model_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building a Deep Learning Model with LSTM deep leatning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the Time Series Data\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select relevant columns\n",
    "time_series_df = train_data[['Date', 'Sales']].sort_values(by='Date')\n",
    "\n",
    "# Set the Date column as the index\n",
    "time_series_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Check if the time series is stationary\n",
    "def check_stationarity(series):\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    result = adfuller(series)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    return result[1] <= 0.05\n",
    "\n",
    "# Check stationarity\n",
    "is_stationary = check_stationarity(time_series_df['Sales'])\n",
    "\n",
    "# Difference the data if not stationary\n",
    "if not is_stationary:\n",
    "    time_series_df['Sales'] = time_series_df['Sales'].diff().dropna()\n",
    "\n",
    "# Check for autocorrelation and partial autocorrelation\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plot_acf(time_series_df['Sales'])\n",
    "plot_pacf(time_series_df['Sales'])\n",
    "plt.show()\n",
    "\n",
    "# Scale data between -1 and 1\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_df['Sales'].values.reshape(-1, 1))\n",
    "\n",
    "# Create supervised learning data\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step), 0]\n",
    "        X.append(a)\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 10\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "# Re\n",
    "shape input to [samples, time steps, features] for LSTM\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and Train the LSTM Model\n",
    "\n",
    "# Define the LSTM model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "model.add(tf.keras.layers.LSTM(50))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=20, batch_size=1, verbose=2)\n",
    "\n",
    "# Save the model\n",
    "model_filename = f'lstm_model-{timestamp}.h5'\n",
    "model.save(model_filename)\n",
    "print(f'LSTM model saved as {model_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model(model_filename)\n",
    "\n",
    "# Prepare data for prediction (e.g., last `time_step` days)\n",
    "last_data = scaled_data[-time_step:].reshape(1, time_step, 1)\n",
    "\n",
    "# Make a prediction\n",
    "predicted_sales = loaded_model.predict(last_data)\n",
    "\n",
    "# Inverse transform the prediction\n",
    "predicted_sales = scaler.inverse_transform(predicted_sales)\n",
    "print(f'Predicted Sales: {predicted_sales[0][0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
